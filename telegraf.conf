[global_tags]
  # dc = "us-east-1" # will tag all metrics with dc=us-east-1
  # rack = "1a"
  ## Environment variables can be used as tags, and throughout the config file
  # user = "$USER"


# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "10s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "µs"), "ms", "s".
  precision = ""

  ## Log at debug level.
  debug = true
  ## Log only error level messages.
  # quiet = false

  ## Log target controls the destination for logs and can be one of "file",
  ## "stderr" or, on Windows, "eventlog".  When set to "file", the output file
  ## is determined by the "logfile" setting.
  # logtarget = "file"

  ## Name of the file to be logged to when using the "file" logtarget.  If set to
  ## the empty string then logs are written to stderr.
  # logfile = ""

  ## The logfile will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.  Logs are rotated only when
  ## written to, if there is no log activity rotation may be delayed.
  # logfile_rotation_interval = "0d"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
  # logfile_rotation_max_size = "0MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
  # logfile_rotation_max_archives = 5

  ## Pick a timezone to use when logging or type 'local' for local time.
  ## Example: America/Chicago
  # log_with_timezone = ""

  ## Override default hostname, if empty use os.Hostname()
  hostname = "telegraf"
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = true

# [[outputs.influxdb_v2]]
#   name_override = "test"
#   urls = ["$INFLUX_URL"]
#   token = "$INFLUX_TOKEN"
#   organization = "$INFLUX_ORG"
#   bucket = "test"
#   user_agent = "telegraf"
#   content_encoding = "gzip"

[[outputs.kafka]]
  ## kafka brokers 的 URLs
  brokers = ["192.168.100.21:9092"]
  ## Kafka topic for producer messages
  topic = "throughputTest"

  ## 此标签的值将用作主题。如果没有设置 'topic'
  ## 选项被使用。
  #topic_tag = ""

  ## 如果为真，'topic_tag' 将从指标中删除。
  # exclude_topic_tag = false

  # # 可选客户端 ID
  client_id = "kafkaTest"

  ## 设置支持的最低Kafka版本。设置此项可以使用新的
  ## Kafka 功能和 API。特别感兴趣的是lz4压缩
  ## 至少需要0.10.0.0版本。
  ## 例如：版本 = "1.1.0"
  #version = ""

  ## 可选主题后缀配置。
  ## 如果省略该部分，则不使用后缀。
  ## 支持以下主题后缀方法：
  ## 测量 - 后缀等于分隔符 + 测量名称
  ## 标记 - 后缀等于分隔符 + 指定标签的值
  ## 与分隔符交错

  ## 后缀等于“_” + 测量名称
  # [outputs.kafka.topic_suffix]
  #   method = "measurement"
  #   separator = "_"

  ## 后缀等于“__”+测量的“foo”标签值。
  ## 如果没有这样的标签，则后缀等于空字符串
  # [outputs.kafka.topic_suffix]
  #   method = "tags"
  #   keys = ["foo"]
  #   separator = "__"

  ## 后缀等于“_”+测量的“foo”和“bar”
  ## 标签值，用“_”分隔。如果没有这样的标签，
  ## 它们的值被视为空字符串。
  # [outputs.kafka.topic_suffix]
  #   method = "tags"
  #   keys = ["foo", "bar"]
  #   separator = "_"

  ## 路由标签在度量上指定一个标签键，其值用作
  ## 消息键。消息键用于确定将消息发送到哪个分区。
  ## 此标签优先于 routing_key 选项。
  routing_tag = "host"

  ## 路由键设置为消息键，用于确定将消息发送到哪个
  ## 分区。此值仅在未设置routing_tag 时使用，或者在未找到路由标签
  ## 中指定的标签时作为备用。
  ## 如果设置为“random”，将为每条消息生成一个随机值。
  ## 未设置时，不添加消息密钥，并且每条消息都被路由到一个随机的分区。
  # # ex: routing_key = "random"
  # # routing_key = "telegraf"
  # routing_key = ""

  ## Compression codec 表示 ## Kafka 在消息中识别的各种压缩编解码器。
  ## 0：无
  ## 1：Gzip
  ## 2：Snappy
  ## 3：LZ4
  ## 4：ZSTD
  compression_codec = 1

  # # Idempotent Writes
  # # 如果启用，则每条消息只写入一个副本。
  # idempotent_writes = false

  ## RequiredAcks 在 Produce Requests 中用于告诉代理在响应之前必须看到多少个
  ##副本确认
  ## 0 ：生产者从不等待代理的确认。
  ## 此选项提供最低延迟但最弱的持久性
  ## 保证（当服务器出现故障时，一些数据将丢失）。

  ## 1 : 生产者在领导副本收到数据后得到确认。这个选项提供了更好的持久性，因为# # 客户端一直等到服务器确认请求成功
  ## （只有写到现在已经死掉的领导者但还没有复制的消息才会丢失）。

  ## -1：生产者在所有同步副本都收到数据后得到确认。此选项提供了最佳的持久性，我们保证只要在同步副本中至少保留一个，就不会丢失任何消息。
  required_acks = -1

  ## 在失败前重试发送指标的最大次数
  ## 直到下一次刷新。
  max_retry = 3

  # # 可选的 TLS 配置
  # tls_ca = "/etc/telegraf/ca.pem"
  # tls_cert = "/etc/telegraf/cert.pem"
  # tls_key = "/etc/telegraf/key.pem"
  # # 使用 TLS 但跳过链和主机验证
  # insecure_skip_verify = false

  # # 连接代理时使用的可选 SOCKS5 代理
  # socks5_enabled = true
  # socks5_address = "127.0.0.1:1080"
  # socks5_username = "alice"
  # socks5_password = "pass123"

  # # 可选的 SASL 配置
  # sasl_username = "kafka"
  # sasl_password = "secret"

  ##可选 SASL：
  ##之一：OAUTHEARER、PLAIN、SCRAM-SHA-256、SCRAM-SHA-512、GSSAPI
  ##（默认为 PLAIN）
  #sasl_mechanism = ""

  # # used if sasl_mechanism is GSSAPI (experimental)
  # sasl_gssapi_service_name = ""
  # ## One of: KRB5_USER_AUTH and KRB5_KEYTAB_AUTH
  # sasl_gssapi_auth_type = "KRB5_USER_AUTH"
  # sasl_gssapi_kerberos_config_path = "/"
  # sasl_gssapi_realm = "realm"
  # sasl_gssapi_key_tab_path = ""
  # sasl_gssapi_disable_pafxfast =错误的

  # # 如果 sasl_mechanism 为 OAUTHBEARER（实验性），则使用
  # sasl_access_token = ""

  ## SASL 协议版本。连接到 Azure EventHub 时设置为 0。
  # sasl_version = 1

  #禁用 Kafka 元数据完整获取
  # metadata_full = false

  # # 要输出的数据格式。
  # # 每种数据格式都有自己独特的配置选项集，请在此处阅读
  # # 更多关于它们的信息：
  # # https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
  data_format = "json"

[[inputs.mqtt_consumer]]
  servers = ["$MQTT_BROKER_URL"]

  topics = [
    "/test/0/A",
  ]
  topic_tag = "topic"

  qos = 0
  connection_timeout = "30s"
  max_undelivered_messages = 10000

  persistent_session = true
  client_id = "test"

  username = "$MQTT_BROKER_USERNAME"
  password = "$MQTT_BROKER_PASSWORD"

  data_format = "json"

  json_name_key = "test"
  tag_keys = ["topic","device_id"]
  json_string_fields= ["loop_b_load_percentage","loop_a_load_percentage","cooling_water_return_temperature","chilled_water_supply_temperature"]

# Convert values to another metric value type
[[processors.converter]]
  order = 1
  [processors.converter.fields]
    measurement = []
    tag = []
    string = []
    integer = ["device_id"]
    unsigned = []
    boolean = []
    float = ["loop_b_load_percentage","loop_a_load_percentage","cooling_water_return_temperature","chilled_water_supply_temperature"]
